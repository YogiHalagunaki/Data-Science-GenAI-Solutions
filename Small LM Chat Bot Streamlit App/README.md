# ğŸ§  SLM Inference Streamlit UI

A lightweight, interactive Streamlit-based UI to manage and chat with small language models (SLMs) running via a local inference API.

## ğŸš€ Features

- âœ… List and manage downloaded models
- ğŸ“¥ Download SLMs from predefined list
- ğŸ’¬ Chat interface with selected model
- âš™ï¸ Customize inference parameters (temperature, top_p, top_k, penalties, etc.)
- ğŸ§¹ Clear chat history with a button click

## ğŸ“¦ Models Available for Download

The following models can be downloaded and used via the UI:
- `llama-3.2-1b` â€“ LLaMA 3.2B (1B variant)
- `qwen-2.5-1.5b` â€“ Qwen 2.5 (1.5B)
- `qwen-2.5-0.5b` â€“ Qwen 2.5 (0.5B)
- `gemma-2-2b` â€“ Gemma 2 (2B)

## ğŸ§° Prerequisites

- Python 3.9+
- A backend REST API running locally at `http://localhost:8000` (must expose endpoints for model management and chat)

## ğŸ“ Project Structure
```bash 
.
â”œâ”€â”€ streamlit_app.py # Main Streamlit app
â”œâ”€â”€ UI_items.py # UI components like headers
â”œâ”€â”€ fastapi_server.py #backend server API
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md # You're here

```

## ğŸ”§ Installation

### 1. Clone the repository:

```bash
git clone https://github.com/YogiHalagunaki/Data-Science-GenAI-Solutions.git
cd Small LM Chat Bot Streamlit App  # or the correct path to your Chat Bot

```
### 2. Create a virtual environment:
```bash 
python -m venv venv
source venv/bin/activate  # On Windows use `venv\Scripts\activate`

```
### 3. Install dependencies:
```bash
pip install -r requirements.txt

```
### 4. Running the FastAPI Server
```bash
python fastapi_server.py

```
By default, the API runs on:
http://localhost:8000

### 5. Running the App
```bash 
streamlit run streamlit_app.py

Note :  Ensure your backend API is running at http://localhost:8000.
```
---
## ğŸ™‹ Author

**Yogi Halagunaki**  
GitHub: [@YogiHalagunaki](https://github.com/YogiHalagunaki)  
Email: halagunakiyogi@gmil.com  
Location: India 
